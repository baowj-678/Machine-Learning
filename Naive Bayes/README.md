# 朴素贝叶斯（Naive Bayes）

### 简介：

**朴素贝叶斯（Naive Bayes）**是基于*贝叶斯定理*和*特征条件独立假设*的分类方法。



#### 条件独立性假设

$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots X^{(n)}=x^{(n)}|Y=c_k)$$

$$=\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)$$

**即**，各个维度的数据之间没有关系。



### 模型：

#### 预测算法

通过习得数据计算**后验概率分布**：

$$P(Y=c_k|X=x)=\frac{P(Y=c_K,X=x)}{P(X=x)}=\frac{P(X=x|Y=c_k)P(Y=c_K)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$

由**条件独立性**得：

$$P(Y=c_l|X=x)=\frac{P(Y=c_K)\prod_iP(X^{(i)}=x^{(i)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_iP(X^{(i)}=x^{(i)}|Y=c_k)}$$

因为分母相同，**所以**

$$y=\arg max_{c_k}P(Y=c_k)\prod_iP(X^{(i)}=x^{(i)}|Y=c_k)$$



#### 学习算法

**极大似然估计**：

$$P(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N},k=1,2,\cdots k$$

**条件概率的极大似然估计**为：

$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}$$

$X_i^{(j)}$：第$i$个样本的第$j$个特征；

$a_{jl}$：第$j$个特征可能取的第$l$个值；

$I$：$0-1$指示函数。



#### 算法

**输入数据**：

$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$：实例数据；

$x_i=(x_i^{(1)},x_i^{(2)},\cdots ,x_i^{(n)})^T$：每个数据为$n$维；

$x_i^{(j)}\in\{a_{j1},a_{j2},\cdots,a_{jS_j}\}$：每个维度的可能取值；

$a_{jl}$：第$j$个特征可能取得第$l$个值；

$y_i\in\{c_1,c_2,\cdots ,c_K\}$：可能取值；

**步骤**：

* 计算**先验概率**和**条件概率**；
* 计算每个可能分类的**后验概率分布**；
* 取概率最大的类；



### 贝叶斯估计：

**原因**：极大似然估计可能出现**概率值为0**的情况。



#### 贝叶斯估计

$$P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}$$

当$\lambda=1$时，称为**拉普拉斯平滑**。



